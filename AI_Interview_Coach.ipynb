{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuz5P0MHtyyvl7ome53ifp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DgXXby8Rf0f"
      },
      "outputs": [],
      "source": [
        "#!pip install streamlit pyngrok langchain chromadb google-generativeai sentence-transformers --quiet\n",
        "#!pip install -U langchain-community\n",
        "#!pip install langchain-google-genai\n",
        "#!pip install streamlit\n",
        "#!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade google-genai"
      ],
      "metadata": {
        "id": "-aPwb9s5Aki0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set API key\n",
        "os.environ['GOOGLE_API_KEY'] = \"\"\n",
        "\n",
        "# Sample questions dataset\n",
        "questions_data = [\n",
        "    {\"role\": \"Data Scientist\", \"question\": \"Explain the difference between supervised and unsupervised learning.\"},\n",
        "    {\"role\": \"Data Scientist\", \"question\": \"What is overfitting and how do you prevent it?\"},\n",
        "    {\"role\": \"ML Engineer\", \"question\": \"Explain gradient descent.\"},\n",
        "    {\"role\": \"ML Engineer\", \"question\": \"How do you optimize a deep learning model?\"}\n",
        "]\n",
        "\n",
        "# Create embeddings and vector store\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_store = Chroma.from_texts(\n",
        "    [q[\"question\"] for q in questions_data],\n",
        "    embedding=embedding_model,\n",
        "    metadatas=[{\"role\": q[\"role\"]} for q in questions_data]\n",
        ")\n",
        "\n",
        "def retrieve_interview_question(role):\n",
        "    results = vector_store.similarity_search(role, k=1)\n",
        "    return results[0].page_content if results else None\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
        "\n",
        "def evaluate_answer_and_suggest_correct(question, user_answer):\n",
        "    template = \"\"\"\n",
        "    Interview Question: {question}\n",
        "    Candidate's Answer: {user_answer}\n",
        "\n",
        "    Please do the following:\n",
        "    1. Evaluate the candidate's answer in detail.\n",
        "    2. Suggest improvements if any.\n",
        "    3. If the answer is incorrect or incomplete or NA, provide the correct model answer separately.\n",
        "\n",
        "    Format your response like this:\n",
        "    Feedback: <your feedback>\n",
        "    Correct Answer (if applicable): <correct answer or 'N/A'>\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"question\": question, \"user_answer\": user_answer})\n",
        "\n",
        "    return response.content\n",
        "\n",
        "# ---------------------------- Streamlit Interface ----------------------------\n",
        "\n",
        "st.title(\"üß† AI-Powered Interview Assistant\")\n",
        "\n",
        "role = st.selectbox(\"Select Role:\", [\"Data Scientist\", \"ML Engineer\"])\n",
        "\n",
        "if st.button(\"Get Interview Question\"):\n",
        "    question = retrieve_interview_question(role)\n",
        "    if question:\n",
        "        st.subheader(\"Interview Question:\")\n",
        "        st.write(question)\n",
        "\n",
        "        user_answer = st.text_area(\"Your Answer:\")\n",
        "\n",
        "        if st.button(\"Evaluate Answer\"):\n",
        "            if user_answer.strip():\n",
        "                feedback = evaluate_answer_and_suggest_correct(question, user_answer)\n",
        "                st.subheader(\"AI Feedback:\")\n",
        "                st.write(feedback)\n",
        "            else:\n",
        "                st.warning(\"Please enter your answer before clicking 'Evaluate Answer'.\")\n",
        "    else:\n",
        "        st.error(\"No question found for this role in the dataset.\")\n"
      ],
      "metadata": {
        "id": "vzZ1ZsfMQlHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install streamlit pyngrok langchain langchain-community langchain-google-genai chromadb google-genai\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Set your ngrok authtoken (replace with your actual token)\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the tunnel\n",
        "public_url = ngrok.connect(port=8501)\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "\n",
        "# Run Streamlit app\n",
        "!streamlit run interview_assistant.py --server.port 8501\n"
      ],
      "metadata": {
        "id": "PVtdGYL-T_6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}